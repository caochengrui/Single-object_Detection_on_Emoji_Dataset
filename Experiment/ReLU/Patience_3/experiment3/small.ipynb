{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Qo4G4VCFWByreK6lRDPsUR30RDXpBVRz","timestamp":1738217267763},{"file_id":"1nBxlAGksoQ2E8EWt8zJnTx1h0nw7oifw","timestamp":1738212938033}],"gpuType":"T4","collapsed_sections":["9C1adRBF8Xz2","9ChtVNxi8hY5","UD1N0_GA-Ryj","DGmOMtE6-Op7","DVyjTTRA_KDP","-fDH5grM_-SH","lEfHhpMWAKgY","b-vUbvLNAOj7","WSeXLQ_wAUKu","SjvDBiOrAknY"],"authorship_tag":"ABX9TyORhEEz36unM4sC3G+pzb+y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Import Libraries and Check Device"],"metadata":{"id":"9C1adRBF8Xz2"}},{"cell_type":"code","source":["import os\n","import json\n","import random\n","import zipfile\n","import shutil\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageDraw\n","import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","import torchvision.transforms as transforms\n","from tqdm import tqdm\n","\n","# Check the available device (GPU or CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"[Info] Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U1UZU9Bh8XBE","executionInfo":{"status":"ok","timestamp":1738220121462,"user_tz":-480,"elapsed":18287,"user":{"displayName":"chengrui cao","userId":"05020108665719249096"}},"outputId":"1654305d-95d0-476f-da78-40d70637819a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] Using device: cuda\n"]}]},{"cell_type":"markdown","source":["# Download and Extract Dataset/small"],"metadata":{"id":"9ChtVNxi8hY5"}},{"cell_type":"code","source":["# GitHub repository URL\n","repo_url = \"https://github.com/caochengrui/Object_Classification_and_Localiztion_on_Emoji_Dataset.git\"\n","repo_dir = \"./Object_Classification_and_Localiztion_on_Emoji_Dataset\"\n","\n","# Clone the repository if it hasn't been cloned yet\n","if not os.path.exists(repo_dir):\n","    print(\"[Info] Cloning repo...\")\n","    !git clone \"$repo_url\" \"$repo_dir\" --depth=1\n","else:\n","    print(\"[Info] Repo already exists, skip cloning.\")\n","\n","# Copy the 'images' and 'annotations' folders from the repository to the current directory\n","if not os.path.exists(\"./images\"):\n","    print(\"[Info] Copying images folder from repo...\")\n","    !cp -r Object_Classification_and_Localiztion_on_Emoji_Dataset/Dataset/small/images ./\n","if not os.path.exists(\"./annotations\"):\n","    print(\"[Info] Copying annotations folder from repo...\")\n","    !cp -r Object_Classification_and_Localiztion_on_Emoji_Dataset/Dataset/small/annotations ./\n","\n","# Unzip the train, validation, and test image archives\n","subsets = [\"train\", \"val\", \"test\"]\n","for subset in subsets:\n","    zip_path = f\"./images/{subset}/{subset}_images.zip\"\n","    if os.path.exists(zip_path):\n","        print(f\"[Info] Unzipping {zip_path} ...\")\n","        with zipfile.ZipFile(zip_path, \"r\") as zf:\n","            zf.extractall(f\"./images/{subset}\")\n","        print(\"Done.\")\n","    else:\n","        print(f\"[Warning] {zip_path} not found, please check your dataset structure.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NGNVDhqb8gOs","executionInfo":{"status":"ok","timestamp":1738220127562,"user_tz":-480,"elapsed":6104,"user":{"displayName":"chengrui cao","userId":"05020108665719249096"}},"outputId":"0b00a7d4-62b9-442c-d530-88e4cf15e535"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] Cloning repo...\n","Cloning into './Object_Classification_and_Localiztion_on_Emoji_Dataset'...\n","remote: Enumerating objects: 58, done.\u001b[K\n","remote: Counting objects: 100% (58/58), done.\u001b[K\n","remote: Compressing objects: 100% (32/32), done.\u001b[K\n","remote: Total 58 (delta 8), reused 55 (delta 8), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (58/58), 78.35 MiB | 39.25 MiB/s, done.\n","Resolving deltas: 100% (8/8), done.\n","[Info] Copying images folder from repo...\n","[Info] Copying annotations folder from repo...\n","[Info] Unzipping ./images/train/train_images.zip ...\n","Done.\n","[Info] Unzipping ./images/val/val_images.zip ...\n","Done.\n","[Info] Unzipping ./images/test/test_images.zip ...\n","Done.\n"]}]},{"cell_type":"markdown","source":["# Define Constants"],"metadata":{"id":"UD1N0_GA-Ryj"}},{"cell_type":"code","source":["IMAGE_SIZE = 144\n","EMOJI_SIZE = 52\n","NUM_CLASSES = 9"],"metadata":{"id":"2MPi1p3J-TDZ","executionInfo":{"status":"ok","timestamp":1738220127562,"user_tz":-480,"elapsed":5,"user":{"displayName":"chengrui cao","userId":"05020108665719249096"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Create Custom Dataset"],"metadata":{"id":"DGmOMtE6-Op7"}},{"cell_type":"code","source":["class SingleEmojiCocoDataset(Dataset):\n","    \"\"\"\n","    Loads images from images/<subset>/*.jpg,\n","    and retrieves (class_id, bbox) from annotations/<subset>/<subset>.json in COCO format.\n","    Only single-object annotations are included => (category_id, bbox).\n","    Outputs: (image_tensor, class_label, coord_label)\n","             where coord_label = [row_norm, col_norm]\n","    \"\"\"\n","    def __init__(self, subset=\"train\", transform=None):\n","        super().__init__()\n","        self.subset = subset\n","        self.transform = transform\n","\n","        # 1) Read COCO JSON annotations\n","        ann_path = os.path.join(\"annotations\", subset, f\"{subset}.json\")\n","        with open(ann_path, \"r\", encoding=\"utf-8\") as f:\n","            coco_data = json.load(f)\n","\n","        self.image_infos = {img[\"id\"]: img for img in coco_data[\"images\"]}\n","        self.annotations = coco_data[\"annotations\"]  # All annotations\n","\n","        # Create a mapping from category_id to 0..8\n","        self.catId_to_label = {}\n","        for cat in coco_data[\"categories\"]:\n","            # cat[\"id\"] in [1..9], cat[\"name\"] ~ 'happy' ...\n","            self.catId_to_label[cat[\"id\"]] = cat[\"id\"] - 1  # => 0..8\n","\n","        # 2) Generate the list of samples\n","        #    For single-object detection, each image corresponds to 1 annotation\n","        self.samples = []\n","        for ann in self.annotations:\n","            img_id = ann[\"image_id\"]\n","            cat_id = ann[\"category_id\"]\n","            bbox = ann[\"bbox\"]   # [x, y, w, h]\n","            # note x=col, y=row in typical COCO\n","            # bounding box pixel\n","            x, y, w, h = bbox\n","\n","            row_norm = y / IMAGE_SIZE\n","            col_norm = x / IMAGE_SIZE\n","\n","            class_label = self.catId_to_label[cat_id]\n","\n","            # file_name\n","            img_info = self.image_infos[img_id]\n","            file_name = img_info[\"file_name\"]\n","\n","            self.samples.append({\n","                \"img_path\": os.path.join(\"images\", subset, file_name),\n","                \"class_label\": class_label,\n","                \"coord_label\": [row_norm, col_norm]\n","            })\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        record = self.samples[idx]\n","        img_path = record[\"img_path\"]\n","        class_label = record[\"class_label\"]\n","        coord_label = record[\"coord_label\"]\n","\n","        # 1) Load image\n","        image = Image.open(img_path).convert(\"RGB\")  # => PIL Image\n","        # 2) Apply transforms (e.g., ToTensor)\n","        if self.transform:\n","            image = self.transform(image)\n","        else:\n","            # Default to tensor conversion\n","            image = transforms.ToTensor()(image)  # => (C,H,W), in [0,1]\n","\n","        # 3) Return the processed data\n","        class_label_t = torch.tensor(class_label, dtype=torch.long)\n","        coord_label_t = torch.tensor(coord_label, dtype=torch.float32)\n","        return image, class_label_t, coord_label_t"],"metadata":{"id":"Sia_e4qJ_CHE","executionInfo":{"status":"ok","timestamp":1738220127562,"user_tz":-480,"elapsed":4,"user":{"displayName":"chengrui cao","userId":"05020108665719249096"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Prepare Data Loaders"],"metadata":{"id":"DVyjTTRA_KDP"}},{"cell_type":"code","source":["# Define transformations for training, validation, and testing\n","train_transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","val_transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","test_transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","# Initialize datasets for training, validation, and testing\n","train_dataset = SingleEmojiCocoDataset(subset=\"train\", transform=train_transform)\n","val_dataset = SingleEmojiCocoDataset(subset=\"val\", transform=val_transform)\n","test_dataset = SingleEmojiCocoDataset(subset=\"test\", transform=test_transform)\n","\n","# Create data loaders with appropriate batch sizes and shuffling\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n","\n","# Print dataset sizes for verification\n","print(f\"[Info] train_dataset size: {len(train_dataset)}\")\n","print(f\"[Info] val_dataset size:   {len(val_dataset)}\")\n","print(f\"[Info] test_dataset size:  {len(test_dataset)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WOlHC8es_Jas","executionInfo":{"status":"ok","timestamp":1738220127562,"user_tz":-480,"elapsed":4,"user":{"displayName":"chengrui cao","userId":"05020108665719249096"}},"outputId":"69948be2-b3c2-467d-9144-78f0e7a458e0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] train_dataset size: 2800\n","[Info] val_dataset size:   800\n","[Info] test_dataset size:  400\n"]}]},{"cell_type":"markdown","source":["# Define Intersection over Union (IoU) Metric"],"metadata":{"id":"-fDH5grM_-SH"}},{"cell_type":"code","source":["class IoU:\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.total_iou = 0.0\n","        self.num_examples = 0\n","\n","    def update(self, y_true, y_pred):\n","        gt_x1 = y_true[:, 1] * IMAGE_SIZE\n","        gt_y1 = y_true[:, 0] * IMAGE_SIZE\n","        gt_x2 = gt_x1 + EMOJI_SIZE\n","        gt_y2 = gt_y1 + EMOJI_SIZE\n","\n","        pred_x1 = y_pred[:, 1] * IMAGE_SIZE\n","        pred_y1 = y_pred[:, 0] * IMAGE_SIZE\n","        pred_x2 = pred_x1 + EMOJI_SIZE\n","        pred_y2 = pred_y1 + EMOJI_SIZE\n","\n","        inter_x1 = torch.max(gt_x1, pred_x1)\n","        inter_y1 = torch.max(gt_y1, pred_y1)\n","        inter_x2 = torch.min(gt_x2, pred_x2)\n","        inter_y2 = torch.min(gt_y2, pred_y2)\n","\n","        inter_w = (inter_x2 - inter_x1).clamp(min=0)\n","        inter_h = (inter_y2 - inter_y1).clamp(min=0)\n","        inter_area = inter_w * inter_h\n","\n","        gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)\n","        pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)\n","        union_area = gt_area + pred_area - inter_area\n","\n","        iou = torch.where(union_area > 0, inter_area / union_area, torch.zeros_like(union_area))\n","        self.total_iou += iou.sum().item()\n","        self.num_examples += y_true.size(0)\n","\n","    def compute(self):\n","        if self.num_examples == 0:\n","            return 0\n","        return self.total_iou / self.num_examples"],"metadata":{"id":"oEjS9x9JAAnm","executionInfo":{"status":"ok","timestamp":1738220127562,"user_tz":-480,"elapsed":3,"user":{"displayName":"chengrui cao","userId":"05020108665719249096"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Define the Model"],"metadata":{"id":"lEfHhpMWAKgY"}},{"cell_type":"code","source":["class ObjectClassificationLocalizationModel(nn.Module):\n","    def __init__(self, num_classes=9):\n","        super(ObjectClassificationLocalizationModel, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size=3, padding=0),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(16),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(16, 32, kernel_size=3, padding=0),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(32),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(32, 64, kernel_size=3, padding=0),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=0),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(128),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, padding=0),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(256),\n","            nn.MaxPool2d(2)\n","        )\n","        self.flatten = nn.Flatten()\n","        self.fc = nn.Sequential(\n","            nn.Linear(256 * 2 * 2, 256),\n","            nn.ReLU()\n","        )\n","\n","        self.classifier = nn.Linear(256, num_classes)\n","        self.regressor = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 2),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.flatten(x)\n","        x = self.fc(x)\n","        class_output = self.classifier(x)\n","        coord_output = self.regressor(x)\n","        return class_output, coord_output\n","\n","# Initialize the model and move it to the appropriate device\n","model = ObjectClassificationLocalizationModel(NUM_CLASSES).to(device)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83uYtUMTALlC","executionInfo":{"status":"ok","timestamp":1738220127928,"user_tz":-480,"elapsed":369,"user":{"displayName":"chengrui cao","userId":"05020108665719249096"}},"outputId":"0a1fcce7-735b-4622-b87c-612dc2a37e3d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["ObjectClassificationLocalizationModel(\n","  (features): Sequential(\n","    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n","    (1): ReLU()\n","    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n","    (5): ReLU()\n","    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n","    (9): ReLU()\n","    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (12): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n","    (13): ReLU()\n","    (14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n","    (17): ReLU()\n","    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (fc): Sequential(\n","    (0): Linear(in_features=1024, out_features=256, bias=True)\n","    (1): ReLU()\n","  )\n","  (classifier): Linear(in_features=256, out_features=9, bias=True)\n","  (regressor): Sequential(\n","    (0): Linear(in_features=256, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=2, bias=True)\n","    (3): Sigmoid()\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":["# Configure Training Parameters"],"metadata":{"id":"b-vUbvLNAOj7"}},{"cell_type":"code","source":["# Define loss functions for classification and coordinate regression\n","criterion_class = nn.CrossEntropyLoss()\n","criterion_coord = nn.MSELoss()\n","\n","# Define optimizers for different parts of the model\n","optimizer_shared = optim.Adam(list(model.features.parameters()) + list(model.fc.parameters()), lr=0.001)\n","optimizer_class  = optim.Adam(model.classifier.parameters(), lr=0.001)\n","optimizer_coord  = optim.Adam(model.regressor.parameters(), lr=0.001)\n","\n","# Define learning rate schedulers based on validation metrics\n","scheduler_class = optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer_class, mode='max', factor=0.5, patience=3, verbose=True, min_lr=1e-6)\n","scheduler_coord = optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer_coord, mode='max', factor=0.5, patience=3, verbose=True, min_lr=1e-6)\n","\n","# Early stopping parameters\n","early_stop_patience = 3\n","num_epochs = 50  # Adjust as needed\n","best_val_acc = 0.0\n","best_val_iou = 0.0\n","epochs_no_improve = 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_Itgx0rARNO","executionInfo":{"status":"ok","timestamp":1738220127928,"user_tz":-480,"elapsed":5,"user":{"displayName":"chengrui cao","userId":"05020108665719249096"}},"outputId":"8f329593-d726-4150-8d01-815f18af49ca"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["# Define Visualization Functions"],"metadata":{"id":"WSeXLQ_wAUKu"}},{"cell_type":"code","source":["def plot_bounding_box(image, gt_coords, pred_coords=None, norm=False):\n","    if isinstance(image, np.ndarray):\n","        image = Image.fromarray(image.astype('uint8'))\n","    draw = ImageDraw.Draw(image)\n","\n","    gt_row, gt_col = gt_coords\n","    if norm:\n","        gt_row *= IMAGE_SIZE\n","        gt_col *= IMAGE_SIZE\n","    draw.rectangle([gt_col, gt_row, gt_col + EMOJI_SIZE, gt_row + EMOJI_SIZE],\n","                   outline='green', width=3)\n","\n","    if pred_coords is not None:\n","        pred_row, pred_col = pred_coords\n","        if norm:\n","            pred_row *= IMAGE_SIZE\n","            pred_col *= IMAGE_SIZE\n","        draw.rectangle([pred_col, pred_row, pred_col + EMOJI_SIZE, pred_row + EMOJI_SIZE],\n","                       outline='red', width=3)\n","\n","    return image\n","\n","def visualize_predictions(model, loader, device, num_samples=4):\n","    model.eval()\n","    images, class_labels, coord_labels = next(iter(loader))\n","    images = images.to(device)\n","    class_labels = class_labels.to(device)\n","    coord_labels = coord_labels.to(device)\n","\n","    with torch.no_grad():\n","        class_preds, coord_preds = model(images)\n","        class_pred_labels = torch.argmax(class_preds, dim=1)\n","\n","    images_cpu = images.cpu().numpy().transpose(0, 2, 3, 1)\n","    class_labels_cpu = class_labels.cpu().numpy()\n","    class_pred_labels_cpu = class_pred_labels.cpu().numpy()\n","    coord_labels_cpu = coord_labels.cpu().numpy()\n","    coord_preds_cpu = coord_preds.cpu().numpy()\n","\n","    fig, axs = plt.subplots(1, num_samples, figsize=(4*num_samples, 4))\n","    for i in range(num_samples):\n","        if i >= len(images_cpu):\n","            break\n","\n","        img = (images_cpu[i]*255).astype('uint8')  # Restore visualization\n","        true_class = class_labels_cpu[i]\n","        pred_class = class_pred_labels_cpu[i]\n","        true_coord = coord_labels_cpu[i]\n","        pred_coord = coord_preds_cpu[i]\n","\n","        img_with_boxes = plot_bounding_box(img, gt_coords=true_coord, pred_coords=pred_coord, norm=True)\n","\n","        title_color = 'green' if (true_class == pred_class) else 'red'\n","        axs[i].imshow(img_with_boxes)\n","        axs[i].set_title(f\"T:{true_class}, P:{pred_class}\", color=title_color)\n","        axs[i].axis('off')\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"AKZ7uRAeAYot","executionInfo":{"status":"ok","timestamp":1738220127928,"user_tz":-480,"elapsed":4,"user":{"displayName":"chengrui cao","userId":"05020108665719249096"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Train the Model"],"metadata":{"id":"SjvDBiOrAknY"}},{"cell_type":"code","source":["# variables to store best epoch info\n","best_epoch = 0\n","best_train_class_loss = float('inf')\n","best_train_coord_loss = float('inf')\n","best_train_class_acc = 0.0\n","best_train_iou = 0.0\n","best_val_class_loss = float('inf')\n","best_val_coord_loss = float('inf')\n","best_val_class_acc = 0.0\n","best_val_iou_metric = 0.0\n","\n","# Record the start time of training\n","start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_class_loss = 0.0\n","    train_coord_loss = 0.0\n","    train_class_correct = 0\n","    train_iou_metric = IoU()\n","\n","    for images_batch, class_labels_batch, coord_labels_batch in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}/{num_epochs}\"):\n","        images_batch = images_batch.to(device)\n","        class_labels_batch = class_labels_batch.to(device)\n","        coord_labels_batch = coord_labels_batch.to(device)\n","\n","        # Classification step\n","        optimizer_shared.zero_grad()\n","        optimizer_class.zero_grad()\n","\n","        outputs_class, _ = model(images_batch)\n","        loss_class = criterion_class(outputs_class, class_labels_batch)\n","        loss_class.backward()\n","        optimizer_shared.step()\n","        optimizer_class.step()\n","\n","        # Coordinate regression step\n","        optimizer_shared.zero_grad()\n","        optimizer_coord.zero_grad()\n","\n","        _, outputs_coord = model(images_batch)\n","        loss_coord = criterion_coord(outputs_coord, coord_labels_batch)\n","        loss_coord.backward()\n","        optimizer_shared.step()\n","        optimizer_coord.step()\n","\n","        train_class_loss += loss_class.item() * images_batch.size(0)\n","        train_coord_loss += loss_coord.item() * images_batch.size(0)\n","\n","        preds = torch.argmax(outputs_class, dim=1)\n","        train_class_correct += (preds == class_labels_batch).sum().item()\n","\n","        train_iou_metric.update(coord_labels_batch, outputs_coord)\n","\n","    total_train_samples = len(train_dataset)\n","    epoch_class_loss = train_class_loss / total_train_samples\n","    epoch_coord_loss = train_coord_loss / total_train_samples\n","    epoch_class_acc = train_class_correct / total_train_samples\n","    epoch_iou = train_iou_metric.compute()\n","\n","    model.eval()\n","    val_class_loss = 0.0\n","    val_coord_loss = 0.0\n","    val_class_correct = 0\n","    val_iou_metric = IoU()\n","\n","    with torch.no_grad():\n","        for images_batch, class_labels_batch, coord_labels_batch in tqdm(val_loader, desc=f\"[Val]   Epoch {epoch+1}/{num_epochs}\"):\n","            images_batch = images_batch.to(device)\n","            class_labels_batch = class_labels_batch.to(device)\n","            coord_labels_batch = coord_labels_batch.to(device)\n","\n","            outputs_class, outputs_coord = model(images_batch)\n","            loss_class = criterion_class(outputs_class, class_labels_batch)\n","            loss_coord = criterion_coord(outputs_coord, coord_labels_batch)\n","\n","            val_class_loss += loss_class.item() * images_batch.size(0)\n","            val_coord_loss += loss_coord.item() * images_batch.size(0)\n","\n","            preds = torch.argmax(outputs_class, dim=1)\n","            val_class_correct += (preds == class_labels_batch).sum().item()\n","\n","            val_iou_metric.update(coord_labels_batch, outputs_coord)\n","\n","    total_val_samples = len(val_dataset)\n","    val_epoch_class_loss = val_class_loss / total_val_samples\n","    val_epoch_coord_loss = val_coord_loss / total_val_samples\n","    val_epoch_class_acc = val_class_correct / total_val_samples\n","    val_epoch_iou = val_iou_metric.compute()\n","\n","    scheduler_class.step(val_epoch_class_acc)\n","    scheduler_coord.step(val_epoch_iou)\n","\n","    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n","    print(f\"Train Class Loss: {epoch_class_loss:.4f} | Train Coord Loss: {epoch_coord_loss:.4f} \"\n","          f\"| Train Acc: {epoch_class_acc:.4f} | Train IoU: {epoch_iou:.4f}\")\n","    print(f\"Val   Class Loss: {val_epoch_class_loss:.4f} | Val   Coord Loss: {val_epoch_coord_loss:.4f} \"\n","          f\"| Val   Acc: {val_epoch_class_acc:.4f} | Val   IoU: {val_epoch_iou:.4f}\")\n","\n","    improved = False\n","    # Check if IoU improved\n","    if val_epoch_iou > best_val_iou:\n","        best_val_iou = val_epoch_iou\n","        improved = True\n","    # Check if Accuracy improved\n","    if val_epoch_class_acc > best_val_acc:\n","        best_val_acc = val_epoch_class_acc\n","        improved = True\n","\n","    # If either improved, record the best epoch stats and save model\n","    if improved:\n","        best_epoch = epoch + 1\n","        best_train_class_loss = epoch_class_loss\n","        best_train_coord_loss = epoch_coord_loss\n","        best_train_class_acc = epoch_class_acc\n","        best_train_iou = epoch_iou\n","        best_val_class_loss = val_epoch_class_loss\n","        best_val_coord_loss = val_epoch_coord_loss\n","        best_val_class_acc = val_epoch_class_acc\n","        best_val_iou_metric = val_epoch_iou\n","        torch.save(model.state_dict(), \"best_model.pth\")\n","        epochs_no_improve = 0\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= early_stop_patience:\n","            print(\"Early stopping!\")\n","            break\n","\n","    visualize_predictions(model, val_loader, device, num_samples=4)\n","\n","end_time = time.time()\n","total_time = end_time - start_time\n","\n","# After the training loop, print the best epoch information\n","print(f\"\\nBest: Epoch [{best_epoch}/{num_epochs}]\")\n","print(f\"      Train Class Loss: {best_train_class_loss:.4f} | Train Coord Loss: {best_train_coord_loss:.4f} \"\n","      f\"| Train Acc: {best_train_class_acc:.4f} | Train IoU: {best_train_iou:.4f}\")\n","print(f\"      Val   Class Loss: {best_val_class_loss:.4f} | Val   Coord Loss: {best_val_coord_loss:.4f} \"\n","      f\"| Val   Acc: {best_val_class_acc:.4f} | Val   IoU: {best_val_iou_metric:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0,"output_embedded_package_id":"1SiLmBlWxACNCzvXyAHQG36gBcDZrqLlq"},"id":"MA2pUnx1AlZL","executionInfo":{"status":"ok","timestamp":1738220230784,"user_tz":-480,"elapsed":102860,"user":{"displayName":"chengrui cao","userId":"05020108665719249096"}},"outputId":"9173192c-59ee-4883-9abd-f923296aa634"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# Evaluate the Model on Test Data"],"metadata":{"id":"eX-Zvb09ArEx"}},{"cell_type":"code","source":["print(\"\\n============== Results ==============\")\n","# Add weights_only=True to eliminate FutureWarning\n","weights = torch.load(\"best_model.pth\", weights_only=True)\n","model.load_state_dict(weights)\n","model.eval()\n","\n","test_class_loss = 0.0\n","test_coord_loss = 0.0\n","test_class_correct = 0\n","test_iou_metric = IoU()\n","\n","with torch.no_grad():\n","    for images_batch, class_labels_batch, coord_labels_batch in tqdm(test_loader, desc=\"[Test] Evaluating\"):\n","        images_batch = images_batch.to(device)\n","        class_labels_batch = class_labels_batch.to(device)\n","        coord_labels_batch = coord_labels_batch.to(device)\n","\n","        outputs_class, outputs_coord = model(images_batch)\n","        loss_class = criterion_class(outputs_class, class_labels_batch)\n","        loss_coord = criterion_coord(outputs_coord, coord_labels_batch)\n","\n","        test_class_loss += loss_class.item() * images_batch.size(0)\n","        test_coord_loss += loss_coord.item() * images_batch.size(0)\n","\n","        preds = torch.argmax(outputs_class, dim=1)\n","        test_class_correct += (preds == class_labels_batch).sum().item()\n","\n","        test_iou_metric.update(coord_labels_batch, outputs_coord)\n","\n","total_test_samples = len(test_dataset)\n","test_epoch_class_loss = test_class_loss / total_test_samples\n","test_epoch_coord_loss = test_coord_loss / total_test_samples\n","test_epoch_class_acc = test_class_correct / total_test_samples\n","test_epoch_iou = test_iou_metric.compute()\n","\n","print(f\"\\nTest Class Loss (CE):  {test_epoch_class_loss:.4f}\")\n","print(f\"Test Coord Loss (MSE): {test_epoch_coord_loss:.4f}\")\n","print(f\"Test Class Accuracy:   {test_epoch_class_acc:.4f}\")\n","print(f\"Test Coord IoU:        {test_epoch_iou:.4f}\")\n","\n","# Output total training time in minutes and seconds\n","minutes, seconds = divmod(int(total_time), 60)\n","print(f\"Total Training Time: {minutes} minutes {seconds} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eLAg5-bUAt0b","executionInfo":{"status":"ok","timestamp":1738220231123,"user_tz":-480,"elapsed":343,"user":{"displayName":"chengrui cao","userId":"05020108665719249096"}},"outputId":"82981240-fd63-4f19-deae-b536dfdf4d79"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============== Results ==============\n"]},{"output_type":"stream","name":"stderr","text":["[Test] Evaluating: 100%|██████████| 25/25 [00:00<00:00, 53.19it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Test Class Loss (CE):  0.0000\n","Test Coord Loss (MSE): 0.0003\n","Test Class Accuracy:   1.0000\n","Test Coord IoU:        0.8672\n","Total Training Time: 1 minutes 42 seconds\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}